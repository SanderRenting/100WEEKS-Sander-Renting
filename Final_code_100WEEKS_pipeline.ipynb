{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_Mn8jk10rrMM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Choose country and run\n"
      ],
      "metadata": {
        "id": "ZVZ5JlW4rsas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The options are 'KEN', 'CIV'. 'UGA', 'GHA', 'RWA'\n",
        "country = 'KEN'"
      ],
      "metadata": {
        "id": "HQMVfuKXrrie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run this code afterwards"
      ],
      "metadata": {
        "id": "_Mn8jk10rrMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install io\n",
        "!pip install --upgrade pandas\n",
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "8skjk92ji6r4",
        "outputId": "a54de4ae-a180-4ff6-be8a-84dc29000a8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement io (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for io\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Using cached pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              },
              "id": "c9a0fdd595cd4dfcb17395dcd6090738"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Using cached xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 1: IMPORTS\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from io import BytesIO\n",
        "import xlsxwriter\n",
        "\n",
        "# Note: This script requires 'xlsxwriter' to be installed for saving multi-sheet Excel files.\n",
        "# You can install it via pip: pip install xlsxwriter\n",
        "\n",
        "# =============================================================================\n",
        "# PART 2: DATA LOADING AND PRE-PROCESSING\n",
        "# =============================================================================\n",
        "df = pd.read_csv('/content/central-tableau-export-2.0.csv')\n",
        "\n",
        "\n",
        "countries = ['GHA', 'RWA', 'UGA', 'CIV', 'KEN']\n",
        "df = df[df['Country'].isin(countries)]\n",
        "\n",
        "df = df[~df['Round'].isin(['Onboarding', '6', '6.0']) & df['Round'].notna()]\n",
        "df['Round'] = pd.to_numeric(df['Round'], errors='coerce').dropna()\n",
        "df = df[df['Round'] % 1 == 0]\n",
        "df['Round'] = df['Round'].astype(int)\n",
        "df = df[df['Round'].isin([0, 1, 2, 3, 100, 102])]\n",
        "df['Round'] = df['Round'].astype(str)\n",
        "df = df.sort_values(by=['Country', 'Groupnr', 'Round'])\n",
        "\n",
        "print(\"Initial 'Round' values found:\", df['Round'].unique())\n",
        "\n",
        "# =============================================================================\n",
        "# PART 3: VARIABLE DEFINITIONS & INITIAL MAPPING\n",
        "# =============================================================================\n",
        "all_columns = [\n",
        "    \"Groupnr\", \"Round\", \"Country\", \"childmortality\", \"childmortalitytime\", *[f\"foodsecurity{i}\" for i in range(1, 10)],\n",
        "    *[f\"foodsecurity{i}freq\" for i in range(1, 10)],\n",
        "    \"fuelcooking\", \"sourcelighting\", \"watersource\", \"timewatersource_1\", \"timewatersourceunit\", \"Toiletfacility\",\n",
        "    \"materialroof\", \"materialfloor\", \"materialwallsext\",\n",
        "    \"assetsmatrix2_7\", \"assetsmatrix2_14\", \"assetsmatrix2_16\", \"assetsmatrix1_23\", \"assetsmatrix3_14\",\n",
        "    \"assetsmatrix3_16\", \"assetsmatrix2_12\", \"assetsmatrix3_22\",\n",
        "    *[f\"HHMschool_{n}\" for n in range(1, 6)], *[f\"HHMschoolnow_{n}\" for n in range(1, 6)],\n",
        "    *[f\"HHMschoolcompl_{n}\" for n in range(1, 6)], *[f\"HHMage_1_{n}\" for n in range(1, 21)], \"school\", \"schoolcompleted\",\n",
        "    \"savinghowmuch_1\", \"savinghowmuch_2\", \"savinghowmuch_3\", \"savingstotal_1\", \"debt\", \"debtamount_1\", \"debtnote\",\n",
        "    *[f\"anxiety{i}\" for i in range(1, 8)],\n",
        "    \"psychwellbeing_1\", \"psychwellbeing_3\", \"psychwellbeing_5\", \"psychwellbeing2_5\", \"jealousy\", \"jealousywhat\",\n",
        "    *[f\"livestocknumbers_{i}\" for i in [1, 13, 3, 4, 5, 6, 11, 8, 9, 7, 2, 10]], \"assetsmatrix1_4\", \"assetsmatrix1_5\",\n",
        "    \"assetsmatrix1_22\", \"assetsmatrix2_7\", \"assetsmatrix2_14\",\n",
        "    \"assetsmatrix2_15\", \"assetsmatrix2_16\", \"assetsmatrix2_8\", \"assetsmatrix3_17\", \"assetsmatrix2_17\",\n",
        "    \"assetsmatrix2_18\", \"assetsmatrix2_19\", \"assetsmatrix2_11\",\n",
        "    \"assetsmatrix2_12\", \"assetsmatrix3_14\", \"assetsmatrix1_23\", \"assetsmatrix3_15\", \"assetsmatrix3_16\",\n",
        "    \"assetsmatrix3_22\", \"assetsmatrix3_23\", \"occupationmain\",\n",
        "    \"ownsland_scto\", \"meetings1\", \"moneywithdraw\", \"moneyproblems\"\n",
        "]\n",
        "\n",
        "columns_available = [col for col in all_columns if col in df.columns]\n",
        "df = df[columns_available]\n",
        "\n",
        "def livestock_normal(val):\n",
        "    if pd.isna(val) or val == 0: return 0\n",
        "    val = int(val)\n",
        "    if val in [1, 2, 3, 4, 5]: return str(val)\n",
        "    elif 6 <= val <= 10: return '6-10'\n",
        "    elif 11 <= val <= 20: return '11-20'\n",
        "    elif val > 20: return '20+'\n",
        "livestock_indices_normal = [1, 13, 3, 4, 8, 9, 7, 2, 10]\n",
        "for i in livestock_indices_normal:\n",
        "    col = f'livestocknumbers_{i}'\n",
        "    if col in df.columns: df[col] = df[col].apply(livestock_normal)\n",
        "\n",
        "def livestock_high(val):\n",
        "    if pd.isna(val) or val == 0: return 0\n",
        "    val = int(val)\n",
        "    if val in [1, 2, 3, 4, 5]: return str(val)\n",
        "    elif 6 <= val <= 10: return '6-10'\n",
        "    elif 11 <= val <= 20: return '11-20'\n",
        "    elif 21 <= val <= 50: return '21-50'\n",
        "    elif val >= 51: return '51+'\n",
        "    else: return 0\n",
        "\n",
        "livestock_indices_high = [5,6,11]\n",
        "for i in livestock_indices_high:\n",
        "    col = f'livestocknumbers_{i}'\n",
        "    if col in df.columns: df[col] = df[col].apply(livestock_high)\n",
        "\n",
        "def mpi_water(row):\n",
        "    if row['timewatersource_1'] > 30: return 1.0\n",
        "    water = row['watersource']\n",
        "    if water in [1, 2, 5, 7, 12]: return 0.0\n",
        "    elif water in [4, 10]: return 0.3\n",
        "    elif water in [3, 6]: return 0.6\n",
        "    elif water in [8, 9, 11]: return 1.0\n",
        "    else: return None\n",
        "if 'timewatersource_1' in df.columns and 'watersource' in df.columns:\n",
        "    df['MPI_water'] = df.apply(mpi_water, axis=1)\n",
        "\n",
        "def MPI_fuel(row):\n",
        "    fuel = row['fuelcooking']\n",
        "    if fuel in [3, 4, 5, 6]: return 0.0\n",
        "    elif fuel in [7]: return 0.3\n",
        "    elif fuel in [2, 10]: return 0.6\n",
        "    elif fuel in [1, 8, 9]: return 1.0\n",
        "    else: return None\n",
        "if 'fuelcooking' in df.columns:\n",
        "    df['MPI_fuel'] = df.apply(MPI_fuel, axis=1)\n",
        "\n",
        "def MPI_electricity(row):\n",
        "    source = row['sourcelighting']\n",
        "    if source in [1, 2, 4, 9, 15]: return 0.0\n",
        "    elif source in [3]: return 0.3\n",
        "    elif source in [5, 7, 8, 10, 13, 14]: return 0.6\n",
        "    elif source in [6, 11, 12]: return 1.0\n",
        "    else: return None\n",
        "if 'sourcelighting' in df.columns:\n",
        "    df['MPI_electricity'] = df.apply(MPI_electricity, axis=1)\n",
        "\n",
        "def MPI_sanitation(row):\n",
        "    toiletfacility = row['Toiletfacility']\n",
        "    if toiletfacility in [1]: return 2\n",
        "    elif toiletfacility in [6]: return 0.2\n",
        "    elif toiletfacility in [5]: return 0.6\n",
        "    elif toiletfacility in [3]: return 1.0\n",
        "    else: return None\n",
        "if 'Toiletfacility' in df.columns:\n",
        "    df['MPI_sanitation'] = df.apply(MPI_sanitation, axis=1)\n",
        "\n",
        "def MPI_floor(row):\n",
        "    material = row['materialfloor']\n",
        "    if material in [5, 6, 4, 9]: return 2\n",
        "    elif material in [3, 8]: return 1\n",
        "    elif material in [1, 2]: return 0\n",
        "    else: return None\n",
        "\n",
        "def MPI_roof(row):\n",
        "    material = row['materialroof']\n",
        "    if material in [4, 2, 3, 8]: return 2\n",
        "    elif material in [5, 7]: return 1\n",
        "    elif material in [1, 9]: return 0\n",
        "    else: return None\n",
        "\n",
        "def MPI_wall(row):\n",
        "    material = row['materialwallsext']\n",
        "    if material in [4, 6, 2, 3, 13]: return 2\n",
        "    elif material in [1, 5, 8, 9, 11]: return 1\n",
        "    elif material in [7, 14]: return 0\n",
        "    else: return None\n",
        "\n",
        "if 'materialwallsext' in df.columns: df['material_walls'] = df.apply(MPI_wall, axis=1)\n",
        "if 'materialfloor' in df.columns: df['material_floor'] = df.apply(MPI_floor, axis=1)\n",
        "if 'materialroof' in df.columns: df['material_roof'] = df.apply(MPI_roof, axis=1)\n",
        "\n",
        "def average_material_score(row):\n",
        "    values = [row.get('material_walls'), row.get('material_floor'), row.get('material_roof')]\n",
        "    valid_values = [v for v in values if v is not None]\n",
        "    return sum(valid_values) / len(valid_values) if valid_values else None\n",
        "df['MPI_house'] = df.apply(average_material_score, axis=1)\n",
        "\n",
        "def update_debtamount(row):\n",
        "    if row['debt'] == 2.0: return 0\n",
        "    elif row['debt'] == 1.0: return row['debtamount_1']\n",
        "    else: return None\n",
        "if 'debt' in df.columns and 'debtamount_1' in df.columns:\n",
        "    df['debtamount_1'] = df.apply(update_debtamount, axis=1)\n",
        "\n",
        "def calculate_school(row, n):\n",
        "    age_col = f'HHMage_1_{n}'\n",
        "    school_col = f'HHMschool_{n}'\n",
        "    schoolcompl_col = f'HHMschoolcompl_{n}'\n",
        "    if pd.isnull(row.get(age_col)) or row.get(age_col) < 10 or pd.isnull(row.get(school_col)): return None\n",
        "    elif row.get(school_col) == 2: return 1\n",
        "    elif pd.isnull(row.get(schoolcompl_col)): return 1\n",
        "    elif row.get(schoolcompl_col) in [1, -88]: return 1\n",
        "    else: return 0\n",
        "for n in range(1, 21):\n",
        "    if f'HHMage_1_{n}' in df.columns and f'HHMschool_{n}' in df.columns and f'HHMschoolcompl_{n}' in df.columns:\n",
        "      df[f'MPI_6yearsofschool_perHHM_{n}'] = df.apply(lambda row: calculate_school(row, n), axis=1)\n",
        "\n",
        "def MPI_6yearsofschool_woman(row):\n",
        "    if pd.isnull(row.get('school')): return None\n",
        "    elif row.get('school') == 2: return 1\n",
        "    elif pd.isnull(row.get('schoolcompleted')): return 1\n",
        "    elif row.get('schoolcompleted') in [1, -88]: return 1\n",
        "    else: return 0\n",
        "if 'school' in df.columns and 'schoolcompleted' in df.columns:\n",
        "    df['MPI_6yearsofschool_woman'] = df.apply(MPI_6yearsofschool_woman, axis=1)\n",
        "\n",
        "def MPI_6yearsofschool_allHHM(row):\n",
        "    education_columns = [col for col in [f'MPI_6yearsofschool_perHHM_{i}' for i in range(1, 21)] + ['MPI_6yearsofschool_woman'] if col in row.index]\n",
        "    if not education_columns or all(pd.isnull(row[col]) for col in education_columns): return None\n",
        "    elif any(row[col] == 0 for col in education_columns): return 0\n",
        "    else: return 1\n",
        "df['MPI_6yearsofschool_allHHM'] = df.apply(MPI_6yearsofschool_allHHM, axis=1)\n",
        "\n",
        "if 'occupationmain' in df.columns:\n",
        "    occupation_scores = {0: 0, 6: 0, 4: 0, 31: 0, 16: 0, 18: 1, 54: 1, 20: 1, 8: 1, 36: 1, 56: 1, 39: 1, 48: 1, 7: 1, 5: 1, 47: 1, 17: 1, 50: 1, 40: 1, 15: 2, 37: 2, 55: 2, 57: 2, 58: 2, 52: 2, 49: 2}\n",
        "    df['occupation_score'] = df['occupationmain'].map(occupation_scores)\n",
        "\n",
        "# =============================================================================\n",
        "# === THIS IS THE CORRECTED FUNCTION ===\n",
        "# =============================================================================\n",
        "def map_school_level_to_score(value):\n",
        "    # Convert value to numeric. If it can't be converted, it becomes NaN.\n",
        "    value = pd.to_numeric(value, errors='coerce')\n",
        "\n",
        "    # The rest of the logic now works safely because `value` is either a number or NaN.\n",
        "    if pd.isnull(value) or value in [-88, 1, 2]:\n",
        "        return 0  # bad\n",
        "    elif value in [3, 4]:\n",
        "        return 1  # medium\n",
        "    elif value >= 5:\n",
        "        return 2  # good\n",
        "    else:\n",
        "        return 0  # fallback for other numbers or cases\n",
        "\n",
        "school_columns = [f\"HHMschoolcompl_{n}\" for n in range(1, 6)] + [\"schoolcompleted\"]\n",
        "for col in school_columns:\n",
        "    if col in df.columns:\n",
        "        df[f\"{col}_score\"] = df[col].apply(map_school_level_to_score)\n",
        "\n",
        "if 'jealousywhat' in df.columns:\n",
        "    def map_jealousy_score(code):\n",
        "        if code in [3, 4, 5, 6, 7]: return 0\n",
        "        elif code in [1, 2, 8]: return 1\n",
        "        elif code == 0: return 2\n",
        "        else: return None\n",
        "    df['jealousywhat_score'] = df['jealousywhat'].apply(map_jealousy_score)\n",
        "\n",
        "# =============================================================================\n",
        "# PART 4: MAP REMAINING & BINARY VARIABLES\n",
        "# =============================================================================\n",
        "print(\"\\nMapping remaining categorical variables to 0, 1, 2 scores...\")\n",
        "\n",
        "livestock_score_map = {0: 0, '1': 1, '2': 1, '3': 1, '4': 1, '5': 1, '6-10': 2, '11-20': 2, '20+': 2, '21-50': 2, '51+': 2}\n",
        "livestock_cols = [f\"livestocknumbers_{i}\" for i in [1, 13, 3, 4, 5, 6, 11, 8, 9, 7, 2, 10]]\n",
        "for col in livestock_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].map(livestock_score_map)\n",
        "\n",
        "if 'debtnote' in df.columns:\n",
        "    debt_reason_map = {1: 0, 2: 0, 4: 0, 7: 0, 9: 0, 3: 1, 5: 1, 6: 2, 8: 2}\n",
        "    df['debtnote_score'] = df['debtnote'].map(debt_reason_map)\n",
        "\n",
        "print(\"\\nMapping binary variables to a 0 (bad) / 1 (good) scale...\")\n",
        "binary_neg = [\n",
        "    \"debt\", \"foodsecurity1\",\"foodsecurity2\", \"foodsecurity3\", \"foodsecurity4\", \"foodsecurity5\",\n",
        "    \"foodsecurity6\", \"foodsecurity7\", \"foodsecurity8\", \"foodsecurity9\", \"childmortality\",\n",
        "    \"jealousy\", \"assetsmatrix1_4\", \"assetsmatrix1_5\", \"assetsmatrix1_22\", \"assetsmatrix2_15\",\n",
        "    \"assetsmatrix2_8\", \"assetsmatrix3_17\", \"assetsmatrix2_17\",\n",
        "    \"assetsmatrix2_18\", \"assetsmatrix2_19\", \"assetsmatrix2_11\",\n",
        "    \"assetsmatrix3_15\", \"assetsmatrix3_23\"\n",
        "]\n",
        "\n",
        "binary_pos = [\n",
        "    \"HHMschoolnow_1\", \"HHMschoolnow_2\", \"HHMschoolnow_3\",\n",
        "    \"HHMschoolnow_4\", \"HHMschoolnow_5\",\n",
        "    \"school\", \"meetings1\", \"moneywithdraw\", \"moneyproblems\"\n",
        "]\n",
        "\n",
        "neg_map = {1.0: 0.0, 2.0: 1.0}\n",
        "for col in binary_neg:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        df[col] = df[col].map(neg_map)\n",
        "        print(f\"Mapped '{col}' (negative binary) to 0/1.\")\n",
        "\n",
        "pos_map = {1.0: 1.0, 2.0: 0.0}\n",
        "for col in binary_pos:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        df[col] = df[col].map(pos_map)\n",
        "        print(f\"Mapped '{col}' (positive binary) to 0/1.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PART 5: SAVE TO EXCEL\n",
        "# =============================================================================\n",
        "output_filename = 'processed_data_with_scores.xlsx'\n",
        "print(f\"\\nSaving the processed DataFrame to '{output_filename}'...\")\n",
        "\n",
        "try:\n",
        "    with pd.ExcelWriter(output_filename, engine='xlsxwriter') as writer:\n",
        "        df.to_excel(writer, sheet_name='Processed_Data', index=False)\n",
        "    print(\"File saved successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Warning: 'xlsxwriter' is not installed. Trying with default engine.\")\n",
        "    try:\n",
        "        df.to_excel(output_filename, index=False)\n",
        "        print(\"File saved successfully with default engine.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while saving the file: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6ltGOGBinC5",
        "outputId": "9c0a5726-086b-4376-aebc-bf45b77be92b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-1136453054.py:16: DtypeWarning: Columns (12,29,36,39,41,48,57,83,84,85,86,87,88,89,90,91,92,435,438,457,466,472,474,479,480,483,487,488,493,496,497,499,500,541,561,563,586,587,591,592,593,597,599,601,602,621,622,624,635,641,679) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/central-tableau-export-2.0.csv')\n",
            "/tmp/ipython-input-1-1136453054.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Round'] = df['Round'].astype(int)\n",
            "/tmp/ipython-input-1-1136453054.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Round'] = df['Round'].astype(str)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial 'Round' values found: ['0' '100' '102' '2' '1' '3']\n",
            "\n",
            "Mapping remaining categorical variables to 0, 1, 2 scores...\n",
            "\n",
            "Mapping binary variables to a 0 (bad) / 1 (good) scale...\n",
            "Mapped 'debt' (negative binary) to 0/1.\n",
            "Mapped 'foodsecurity1' (negative binary) to 0/1.\n",
            "Mapped 'foodsecurity2' (negative binary) to 0/1.\n",
            "Mapped 'foodsecurity3' (negative binary) to 0/1.\n",
            "Mapped 'foodsecurity4' (negative binary) to 0/1.\n",
            "Mapped 'foodsecurity5' (negative binary) to 0/1.\n",
            "Mapped 'foodsecurity6' (negative binary) to 0/1.\n",
            "Mapped 'foodsecurity7' (negative binary) to 0/1.\n",
            "Mapped 'foodsecurity8' (negative binary) to 0/1.\n",
            "Mapped 'foodsecurity9' (negative binary) to 0/1.\n",
            "Mapped 'childmortality' (negative binary) to 0/1.\n",
            "Mapped 'jealousy' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix1_4' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix1_5' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix1_22' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix2_15' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix2_8' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix3_17' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix2_17' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix2_18' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix2_19' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix2_11' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix3_15' (negative binary) to 0/1.\n",
            "Mapped 'assetsmatrix3_23' (negative binary) to 0/1.\n",
            "Mapped 'HHMschoolnow_1' (positive binary) to 0/1.\n",
            "Mapped 'HHMschoolnow_2' (positive binary) to 0/1.\n",
            "Mapped 'HHMschoolnow_3' (positive binary) to 0/1.\n",
            "Mapped 'HHMschoolnow_4' (positive binary) to 0/1.\n",
            "Mapped 'HHMschoolnow_5' (positive binary) to 0/1.\n",
            "Mapped 'school' (positive binary) to 0/1.\n",
            "Mapped 'meetings1' (positive binary) to 0/1.\n",
            "Mapped 'moneywithdraw' (positive binary) to 0/1.\n",
            "Mapped 'moneyproblems' (positive binary) to 0/1.\n",
            "\n",
            "Saving the processed DataFrame to 'processed_data_with_scores.xlsx'...\n",
            "File saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-BaFE4XyifTL",
        "outputId": "d8016e8a-471a-4d5f-8c26-002cf9f7bf60"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-623629137.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_data_with_scores.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# --- Variable Definitions ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m numerical = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         data = io.parse(\n\u001b[0m\u001b[1;32m    509\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[1;32m   1614\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# doctest: +SKIP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m         \"\"\"\n\u001b[0;32m-> 1616\u001b[0;31m         return self._reader.parse(\n\u001b[0m\u001b[1;32m   1617\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0mfile_rows_needed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sheet_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_rows_needed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"close\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m                 \u001b[0;31m# pyxlsb opens two TemporaryFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36mget_sheet_data\u001b[0;34m(self, sheet, file_rows_needed)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mScalar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mlast_row_with_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0mconverted_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mconverted_row\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconverted_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openpyxl/worksheet/_read_only.py\u001b[0m in \u001b[0;36m_cells_by_row\u001b[0;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[1;32m     83\u001b[0m                                      timedelta_formats=self.parent._timedelta_formats)\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_row\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_row\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openpyxl/worksheet/_reader.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add a finaliser to close the source when this becomes possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mtag_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36miterator\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;31m# load event buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mread_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "df = pd.read_excel('processed_data_with_scores.xlsx')\n",
        "\n",
        "# --- Variable Definitions ---\n",
        "numerical = [\n",
        "    \"savinghowmuch_1\", \"savinghowmuch_2\", \"savinghowmuch_3\",\n",
        "    \"savingstotal_1\", \"debtamount_1\", \"timewatersource_1\"\n",
        "]\n",
        "ordered_categorical = [\n",
        "    *[f\"foodsecurity{i}freq\" for i in range(1, 10)],\n",
        "    *[f\"anxiety{i}\" for i in range(1, 8)],\n",
        "    \"psychwellbeing_1\", \"psychwellbeing_3\", \"psychwellbeing_5\", \"psychwellbeing2_5\"\n",
        "]\n",
        "categorical = [\n",
        "    \"fuelcooking\", \"sourcelighting\", \"watersource\", \"Toiletfacility\",\n",
        "    \"materialroof\", \"materialfloor\", \"materialwallsext\",\n",
        "    *[f\"HHMschoolcompl_{n}\" for n in range(1, 6)],\n",
        "    \"schoolcompleted\", \"livestocknumbers_1\",\n",
        "    *[f\"livestocknumbers_{i}\" for i in [1, 13, 3, 4, 5, 6, 11, 8, 9, 7, 2, 10]],\n",
        "    \"occupationmain\"\n",
        "]\n",
        "binary = [\n",
        "    \"childmortality\",\n",
        "    *[f\"foodsecurity{i}\" for i in range(1, 10)],\n",
        "    *[f\"HHMschool_{n}\" for n in range(1, 6)],\n",
        "    *[f\"HHMschoolnow_{n}\" for n in range(1, 6)],\n",
        "    \"school\", \"debt\", \"jealousy\",\n",
        "    \"assetsmatrix1_4\", \"assetsmatrix1_5\", \"assetsmatrix1_22\",\n",
        "    \"assetsmatrix2_15\", \"assetsmatrix2_8\", \"assetsmatrix3_17\",\n",
        "    \"assetsmatrix2_17\", \"assetsmatrix2_18\", \"assetsmatrix2_19\",\n",
        "    \"assetsmatrix2_11\", \"assetsmatrix3_15\", \"assetsmatrix3_23\",\n",
        "    \"meetings1\", \"moneywithdraw\", \"moneyproblems\"\n",
        "]\n",
        "multiple_choice = [\"debtnote\", \"jealousywhat\"]\n",
        "information = [\"Country\", \"Groupnr\", \"Round\"]\n",
        "\n",
        "\n",
        "# --- Benchmark Calculation Function (As Provided) ---\n",
        "def calculate_benchmarks(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Calculates benchmark statistics (mean/proportion) for all variables per round.\"\"\"\n",
        "    round_to_benchmark_map = {0: \"benchmark_baseline\", 1: \"benchmark_phone_survey_1\", 2: \"benchmark_phone_survey_2\",\n",
        "                              3: \"benchmark_phone_survey_3\", 100: \"benchmark_endline\",\n",
        "                              102: \"benchmark_post-program_survey_2\"}\n",
        "    binary_neg = [\"debt\", *[f\"foodsecurity{i}\" for i in range(1, 10)], \"childmortality\", \"jealousy\", \"assetsmatrix1_4\",\n",
        "                  \"assetsmatrix1_5\", \"assetsmatrix1_22\", \"assetsmatrix2_15\", \"assetsmatrix2_8\", \"assetsmatrix3_17\",\n",
        "                  \"assetsmatrix2_17\", \"assetsmatrix2_18\", \"assetsmatrix2_19\", \"assetsmatrix2_11\", \"assetsmatrix3_15\",\n",
        "                  \"assetsmatrix3_23\"]\n",
        "    binary_pos = [*[f\"HHMschoolnow_{n}\" for n in range(1, 6)], \"school\", \"meetings1\", \"moneywithdraw\", \"moneyproblems\"]\n",
        "\n",
        "    df_work = df.copy()\n",
        "    df_work['Round'] = pd.to_numeric(df_work['Round'], errors='coerce')\n",
        "    df_work.dropna(subset=['Round'], inplace=True)\n",
        "    df_work['Round'] = df_work['Round'].astype(int)\n",
        "    df_work = df_work[df_work['Round'].isin(round_to_benchmark_map.keys())]\n",
        "\n",
        "    all_benchmark_cols = list(set(numerical + ordered_categorical + binary))\n",
        "    for col in all_benchmark_cols:\n",
        "        if col in df_work.columns: df_work[col] = pd.to_numeric(df_work[col], errors='coerce')\n",
        "\n",
        "    agg_dict = {}\n",
        "    for col in list(set(numerical + ordered_categorical)):\n",
        "        if col in df_work.columns: agg_dict[col] = 'mean'\n",
        "    for col in binary_pos:\n",
        "        if col in df_work.columns: agg_dict[col] = lambda s: (s == 1).sum() / s.isin(\n",
        "            [1, 2]).sum() if s.isin([1, 2]).sum() > 0 else np.nan\n",
        "    for col in binary_neg:\n",
        "        if col in df_work.columns: agg_dict[col] = lambda s: (s == 2).sum() / s.isin(\n",
        "            [1, 2]).sum() if s.isin([1, 2]).sum() > 0 else np.nan\n",
        "\n",
        "    benchmarks_df = df_work.groupby('Round').agg(agg_dict)\n",
        "    benchmarks_df = benchmarks_df.rename(index=round_to_benchmark_map)\n",
        "\n",
        "    ordered_cols = [c for c in numerical + ordered_categorical + binary_pos + binary_neg if c in benchmarks_df.columns]\n",
        "    return benchmarks_df[ordered_cols]\n",
        "\n",
        "\n",
        "# --- Plotting & Data Prep Functions (Unchanged) ---\n",
        "def plot_groups_vs_overall(df, groups, variables, dpi=120):\n",
        "    round_order = [0, 1, 2, 3, 100, 102]\n",
        "    labels = {0: 'Baseline', 1: 'Phone survey 1', 2: 'Phone survey 2', 3: 'Phone survey 3', 100: 'Endline',\n",
        "              102: 'Post-program survey'}\n",
        "    df = df.copy();\n",
        "    df['Round'] = df['Round'].astype(int)\n",
        "    plt.style.use('ggplot');\n",
        "    cmap = plt.colormaps['tab10']\n",
        "    buffers = {}\n",
        "    for var in variables:\n",
        "        if var not in df.columns or df[var].dropna().empty: continue\n",
        "        fig, ax = plt.subplots(figsize=(10, 6), dpi=dpi)\n",
        "        overall_stats = df.groupby('Round')[var].agg(mean='mean', q1=lambda x: x.quantile(0.25),\n",
        "                                                     q3=lambda x: x.quantile(0.75)).reindex(round_order).reset_index()\n",
        "        overall_stats['lbl'] = overall_stats['Round'].map(labels)\n",
        "        overall_stats['iqr'] = overall_stats['q3'] - overall_stats['q1']\n",
        "        overall_stats['lower_bound'] = overall_stats['q1'] - 1.5 * overall_stats['iqr']\n",
        "        overall_stats['upper_bound'] = overall_stats['q3'] + 1.5 * overall_stats['iqr']\n",
        "        x_positions = np.arange(len(round_order))\n",
        "        ax.fill_between(x_positions, overall_stats['lower_bound'], overall_stats['upper_bound'], color='lightgray',\n",
        "                        alpha=0.6, label='Overall IQR ±1.5')\n",
        "        ax.plot(x_positions, overall_stats['lower_bound'], 'k-', lw=0.8);\n",
        "        ax.plot(x_positions, overall_stats['upper_bound'], 'k-', lw=0.8)\n",
        "        ax.plot(x_positions, overall_stats['mean'], linestyle='--', marker='o', color='tab:red', linewidth=2,\n",
        "                markersize=8, label='Overall average')\n",
        "        for i, grp in enumerate(groups):\n",
        "            sub = df[df['Groupnr'] == grp][['Round', var]].dropna()\n",
        "            if sub.empty: continue\n",
        "            pts = sorted(zip(sub['Round'], sub[var]), key=lambda x: round_order.index(x[0]))\n",
        "            rounds, vals = zip(*pts)\n",
        "            x_group_positions = [round_order.index(r) for r in rounds]\n",
        "            for j in range(1, len(rounds)):\n",
        "                idx0, idx1 = round_order.index(rounds[j - 1]), round_order.index(rounds[j])\n",
        "                style = '-' if idx1 - idx0 == 1 else 'dotted'\n",
        "                ax.plot(x_positions[[idx0, idx1]], [vals[j - 1], vals[j]], linestyle=style, color=cmap(i),\n",
        "                        linewidth=1.5)\n",
        "            ax.scatter(x_group_positions, vals, marker='s', s=50, color=cmap(i), label=f'Group {grp}')\n",
        "        ax.set_title(f'{var} trend for groups vs overall (with IQR band)', fontsize=16, pad=12)\n",
        "        ax.set_xlabel('Survey Round', fontsize=14, labelpad=8);\n",
        "        ax.set_ylabel(var, fontsize=14, labelpad=8)\n",
        "        ax.set_xticks(x_positions);\n",
        "        ax.set_xticklabels([labels[r] for r in round_order], rotation=45, ha='right')\n",
        "        ax.grid(alpha=0.3);\n",
        "        ax.spines['top'].set_visible(False);\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1));\n",
        "        plt.tight_layout()\n",
        "        buf = io.BytesIO();\n",
        "        fig.savefig(buf, format='png', bbox_inches='tight');\n",
        "        plt.close(fig);\n",
        "        buf.seek(0)\n",
        "        buffers[var] = buf\n",
        "    return buffers\n",
        "\n",
        "\n",
        "def growth_plot(df: pd.DataFrame, groups: list[str], var: str, window: int = 3) -> io.BytesIO:\n",
        "    \"\"\"\n",
        "    Generates a plot showing growth outliers for specified groups against the overall trend.\n",
        "    This version ensures that lines connecting a group's data points are always solid.\n",
        "    \"\"\"\n",
        "    round_order = [0, 1, 2, 3, 100, 102]\n",
        "    labels = {0: 'Baseline', 1: 'Phone survey 1', 2: 'Phone survey 2', 3: 'Phone survey 3', 100: 'Endline',\n",
        "              102: 'Post-program survey'}\n",
        "    x = np.arange(len(round_order))\n",
        "    xlabs = [labels[r] for r in round_order]\n",
        "\n",
        "    ewma_col = f\"{var}_EWMA_{window}_growth\"\n",
        "    if ewma_col not in df.columns:\n",
        "        return None\n",
        "\n",
        "    stats = df.groupby('Round')[ewma_col].agg(\n",
        "        Q1=lambda x: x.quantile(0.25),\n",
        "        Q3=lambda x: x.quantile(0.75)\n",
        "    ).reindex(round_order)\n",
        "\n",
        "    iqr = stats['Q3'] - stats['Q1']\n",
        "    lower = stats['Q1'] - 1.5 * iqr\n",
        "    upper = stats['Q3'] + 1.5 * iqr\n",
        "    raw_gc = f\"{var}_growth\"\n",
        "    overall = df.groupby('Round')[raw_gc].mean().reindex(round_order)\n",
        "\n",
        "    plt.style.use('ggplot')\n",
        "    fig, ax = plt.subplots(figsize=(10, 5), dpi=120)\n",
        "\n",
        "    # Plot Overall Stats (IQR Band and Average Growth)\n",
        "    ax.fill_between(x, lower, upper, color='lightgray', alpha=0.6, label='EWMA IQR ±1.5')\n",
        "    ax.plot(x, lower, 'k-', lw=1)\n",
        "    ax.plot(x, upper, 'k-', lw=1)\n",
        "    ax.plot(x, overall.values, 'o--', color='red', lw=2, ms=6, label='Avg growth')\n",
        "\n",
        "    # Setup legend for groups\n",
        "    cmap = plt.get_cmap('tab10')\n",
        "    for i, gid in enumerate(groups):\n",
        "        ax.plot([], [], color=cmap(i % 10), marker='s', linestyle='-', linewidth=1.5,\n",
        "                label=f\"Group {gid}\")\n",
        "\n",
        "    # --- CORRECTED PLOTTING LOGIC FOR GROUPS ---\n",
        "    def plot_group(df_group, color):\n",
        "        \"\"\"Plots a single group's data with solid lines.\"\"\"\n",
        "        # Get the y-values, reindexing to ensure alignment with all possible rounds\n",
        "        y = (df_group.set_index('Round')[raw_gc].reindex(round_order).values)\n",
        "\n",
        "        # Filter out rounds where this group has no data\n",
        "        valid_indices = ~np.isnan(y)\n",
        "        x_valid, y_valid = x[valid_indices], y[valid_indices]\n",
        "\n",
        "        # Plot the valid points with a solid connecting line\n",
        "        ax.plot(x_valid, y_valid, color=color, lw=1.5, linestyle='-')\n",
        "        ax.scatter(x_valid, y_valid, color=color, s=60, marker='s')\n",
        "\n",
        "    # Iterate through specified groups and plot their data\n",
        "    for i, gid in enumerate(groups):\n",
        "        df_group = df[df['Groupnr'] == gid].copy()\n",
        "        if not df_group.empty and raw_gc in df_group.columns:\n",
        "            plot_group(df_group, cmap(i % 10))\n",
        "\n",
        "    # Final plot styling\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(xlabs, rotation=45, ha='right')\n",
        "    ax.set_ylabel(f\"{var}_growth (Δ)\")\n",
        "    ax.set_title(f\"Growth outliers: {var}\")\n",
        "    ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot to buffer\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png', bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    buf.seek(0)\n",
        "    return buf\n",
        "\n",
        "\n",
        "def add_ewma_columns(df: pd.DataFrame, variables: list[str] | None = None, span: int = 3, adjust: bool = True,\n",
        "                     min_periods: int = 1) -> pd.DataFrame:\n",
        "    df_ewma = df.copy()\n",
        "    if variables is None: variables = [col for col in df_ewma.select_dtypes(include='number').columns.tolist() if\n",
        "                                       col != 'Round']\n",
        "    for var in variables: df_ewma[f\"{var}_EWMA_{span}\"] = df_ewma.groupby('Groupnr')[var].transform(\n",
        "        lambda s: s.ewm(span=span, adjust=adjust, min_periods=min_periods).mean())\n",
        "    return df_ewma\n",
        "\n",
        "\n",
        "def add_growth_to_smoothed_with_interpolation(df: pd.DataFrame, col: str, group_key: str = 'Groupnr') -> pd.Series:\n",
        "    interp = df.groupby(group_key)[col].transform(lambda s: s.interpolate(method='linear', limit_area=None, limit=2))\n",
        "    return interp.groupby(df[group_key]).diff()\n",
        "\n",
        "\n",
        "def df_prep(df: pd.DataFrame, variables: list[str]) -> pd.DataFrame:\n",
        "    required_cols = ['Groupnr', 'Country', 'Round'] + variables\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        variables = [col for col in variables if col not in missing_cols]\n",
        "        required_cols = ['Groupnr', 'Country', 'Round'] + variables\n",
        "    df2 = df[required_cols].copy()\n",
        "    df2 = df2[~df2['Round'].isin(['Onboarding', '6', '6.0']) & df2['Round'].notna()]\n",
        "    df2['Round'] = pd.to_numeric(df2['Round'], errors='coerce')\n",
        "    df2 = df2[df2['Round'] % 1 == 0];\n",
        "    df2['Round'] = df2['Round'].astype(int)\n",
        "    df2 = df2[df2['Round'].isin([0, 1, 2, 3, 100, 102])].sort_values('Round').reset_index(drop=True)\n",
        "    numeric_like_vars = list(\n",
        "        dict.fromkeys([v for v in variables if v not in ['Groupnr', 'Country', 'Round', 'livestocknumbers_1']]))\n",
        "    for col in numeric_like_vars:\n",
        "        if col in df2.columns: df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
        "    df_tr = df2.groupby(['Groupnr', 'Round'])[numeric_like_vars].mean().reset_index().sort_values(\n",
        "        ['Groupnr', 'Round']).reset_index(drop=True).copy()\n",
        "    df_tr = add_ewma_columns(df_tr, variables=numeric_like_vars, span=3, min_periods=1)\n",
        "    new_cols_data = {}\n",
        "    for var in numeric_like_vars:\n",
        "        col = f'{var}_EWMA_3'\n",
        "        if col in df_tr.columns: new_cols_data[f'{col}_growth'] = add_growth_to_smoothed_with_interpolation(df_tr, col)\n",
        "    for var in numeric_like_vars:\n",
        "        new_cols_data[f'{var}_growth'] = df_tr.groupby('Groupnr')[var].diff()\n",
        "        if var in df_tr.columns: new_cols_data[f'{var}_pct_growth'] = df_tr.groupby('Groupnr')[var].pct_change(\n",
        "            fill_method=None) * 100\n",
        "    if new_cols_data: df_tr = pd.concat([df_tr, pd.DataFrame(new_cols_data, index=df_tr.index)], axis=1)\n",
        "    df_tr = df_tr.sort_values(['Groupnr', 'Round']).reset_index(drop=True)\n",
        "    return df_tr\n",
        "\n",
        "\n",
        "def final_prep(raw_df, variables, groups, country, window=3):\n",
        "    df_all = df_prep(raw_df, variables)\n",
        "    df_all = df_all.drop_duplicates(subset=['Groupnr', 'Round'])\n",
        "    if country: df_all = df_all[df_all['Groupnr'].str.startswith(country)]\n",
        "    iqr_bands, avg_growth = {}, {}\n",
        "    for var in variables:\n",
        "        raw_gc, ewma_gc = f\"{var}_growth\", f\"{var}_EWMA_{window}_growth\"\n",
        "        if ewma_gc not in df_all.columns or raw_gc not in df_all.columns: continue\n",
        "        if df_all[ewma_gc].dropna().empty:\n",
        "            lower, upper = pd.Series(np.nan), pd.Series(np.nan)\n",
        "        else:\n",
        "            q1_ewma, q3_ewma = df_all.groupby('Round')[ewma_gc].quantile(0.25), df_all.groupby('Round')[\n",
        "                ewma_gc].quantile(0.75)\n",
        "            iqr_ewma = q3_ewma - q1_ewma\n",
        "            lower, upper = q1_ewma - 1.5 * iqr_ewma, q3_ewma + 1.5 * iqr_ewma\n",
        "        iqr_bands[var] = pd.DataFrame({'lower': lower, 'upper': upper})\n",
        "        if df_all[raw_gc].dropna().empty:\n",
        "            avg_growth[var] = pd.Series(np.nan)\n",
        "        else:\n",
        "            avg_growth[var] = df_all.groupby('Round')[raw_gc].mean()\n",
        "    df_filt = df_all[df_all['Groupnr'].isin(groups)]\n",
        "    flags = {var: {} for var in variables}\n",
        "    for var in variables:\n",
        "        if var not in iqr_bands or var not in avg_growth: continue\n",
        "        band = iqr_bands[var].reset_index().rename(columns={'index': 'Round'})\n",
        "        raw_gc = f\"{var}_growth\"\n",
        "        if raw_gc not in df_filt.columns: continue\n",
        "        tmp = df_filt[['Groupnr', 'Round', raw_gc]].merge(band, on='Round', how='left')\n",
        "        for _, r in tmp.iterrows():\n",
        "            v = r[raw_gc]\n",
        "            if pd.isna(v) or pd.isna(r['lower']) or pd.isna(r['upper']): continue\n",
        "            if v < r['lower']:\n",
        "                flags[var][(r['Groupnr'], r['Round'])] = 'NEG'\n",
        "            elif v > r['upper']:\n",
        "                flags[var][(r['Groupnr'], r['Round'])] = 'POS'\n",
        "    rows = [{'Groupnr': grp, 'Round': rnd, 'variable': var, 'flag': tag} for var, vf in flags.items() for\n",
        "            (grp, rnd), tag in vf.items()]\n",
        "    df_flags = pd.DataFrame(rows)\n",
        "    if not df_flags.empty:\n",
        "        df_out = df_flags.pivot(index=['Groupnr', 'Round'], columns='variable', values='flag').reset_index()\n",
        "    else:\n",
        "        df_out = pd.DataFrame(columns=['Groupnr', 'Round'] + variables)\n",
        "    return df_out, flags, df_all\n",
        "\n",
        "\n",
        "# --- NEW: Function to calculate outliers based on static values within a round ---\n",
        "def calculate_round_outliers(df: pd.DataFrame, num_bin_vars: list[str], cat_vars: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates outliers based on static values within each round using the IQR method.\n",
        "    - For numerical/binary variables, it uses the group's mean.\n",
        "    - For categorical variables, it uses the percentage for each category.\n",
        "    \"\"\"\n",
        "    all_outliers = []\n",
        "    df_work = df.copy()\n",
        "    df_work['Round'] = pd.to_numeric(df_work['Round'], errors='coerce').astype('Int64')\n",
        "\n",
        "    # Ensure relevant columns are numeric\n",
        "    for col in num_bin_vars:\n",
        "        if col in df_work.columns:\n",
        "            df_work[col] = pd.to_numeric(df_work[col], errors='coerce')\n",
        "\n",
        "    rounds = sorted(df_work['Round'].dropna().unique())\n",
        "\n",
        "    # --- Part 1: Numerical and Binary Variables ---\n",
        "    for var in num_bin_vars:\n",
        "        if var not in df_work.columns: continue\n",
        "\n",
        "        # Calculate group means for the variable across all rounds\n",
        "        group_means = df_work.groupby(['Round', 'Groupnr'])[var].mean().reset_index()\n",
        "\n",
        "        for r in rounds:\n",
        "            round_means = group_means[group_means['Round'] == r][var]\n",
        "            if len(round_means) < 4: continue  # Not enough data for IQR\n",
        "\n",
        "            Q1 = round_means.quantile(0.25)\n",
        "            Q3 = round_means.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            if IQR == 0: continue\n",
        "\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            outliers = group_means[\n",
        "                (group_means['Round'] == r) &\n",
        "                ((group_means[var] < lower_bound) | (group_means[var] > upper_bound))\n",
        "                ]\n",
        "\n",
        "            for _, row in outliers.iterrows():\n",
        "                flag = \"HIGH\" if row[var] > Q3 else \"LOW\"\n",
        "                description = f\"{var} ({flag} mean: {row[var]:.2f})\"\n",
        "                all_outliers.append({\n",
        "                    'Groupnr': row['Groupnr'], 'Round': r, 'flag_description': description\n",
        "                })\n",
        "\n",
        "    # --- Part 2: Categorical Variables ---\n",
        "    for var in cat_vars:\n",
        "        if var not in df_work.columns or df_work[var].nunique() < 2: continue\n",
        "\n",
        "        for r in rounds:\n",
        "            df_round = df_work[df_work['Round'] == r].dropna(subset=[var])\n",
        "            if df_round.empty: continue\n",
        "\n",
        "            # Calculate percentage of each category per group\n",
        "            cross_tab = pd.crosstab(index=df_round['Groupnr'], columns=df_round[var], normalize='index') * 100\n",
        "            if cross_tab.shape[0] < 4: continue\n",
        "\n",
        "            # Check each category for outliers\n",
        "            for category in cross_tab.columns:\n",
        "                cat_percentages = cross_tab[category]\n",
        "\n",
        "                Q1 = cat_percentages.quantile(0.25)\n",
        "                Q3 = cat_percentages.quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                if IQR == 0: continue\n",
        "\n",
        "                # We are primarily interested in unusually high percentages\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "                outliers = cat_percentages[cat_percentages > upper_bound]\n",
        "\n",
        "                for group, pct_val in outliers.items():\n",
        "                    description = f\"{var}: {category} (HIGH %: {pct_val:.0f}%)\"\n",
        "                    all_outliers.append({\n",
        "                        'Groupnr': group, 'Round': r, 'flag_description': description\n",
        "                    })\n",
        "\n",
        "    if not all_outliers:\n",
        "        return pd.DataFrame()\n",
        "    return pd.DataFrame(all_outliers)\n",
        "\n",
        "\n",
        "# --- FINALIZED: xlsx file generator with clustering instructions ---\n",
        "def xlsx_generator(raw_df, variables, country, groups, window=3):\n",
        "    print(\"Step 1: Preparing data and identifying trend outliers...\")\n",
        "\n",
        "    all_vars = variables\n",
        "\n",
        "    vars_to_exclude = (\n",
        "    [f\"foodsecurity{i}freq\" for i in range(1, 10)] +\n",
        "    [f\"foodsecurity{i}\" for i in range(1, 10)] +\n",
        "    [f\"livestocknumbers_{i}\" for i in [1, 13, 3, 4, 5, 6, 11, 8, 9, 7, 2, 10]] +\n",
        "    [f\"HHMschool_{n}\" for n in range(1, 6)] +\n",
        "    [f\"HHMschoolnow_{n}\" for n in range(1, 6)] +\n",
        "    [f\"HHMschoolcompl_{n}\" for n in range(1, 6)] +\n",
        "    [\"assetsmatrix1_4\", \"assetsmatrix1_5\", \"assetsmatrix1_22\",\n",
        "    \"assetsmatrix2_15\", \"assetsmatrix2_8\", \"assetsmatrix3_17\",\n",
        "    \"assetsmatrix2_17\", \"assetsmatrix2_18\", \"assetsmatrix2_19\",\n",
        "    \"assetsmatrix2_11\", \"assetsmatrix3_15\", \"assetsmatrix3_23\"],\n",
        "    ['meetings1']\n",
        "    )\n",
        "\n",
        "\n",
        "    trend_vars = [var for var in all_variables if var not in vars_to_exclude]\n",
        "\n",
        "    _, flags, df_all = final_prep(raw_df, trend_vars, groups, country, window)\n",
        "\n",
        "    print(\"Step 2: Calculating benchmark statistics...\")\n",
        "    country_df = raw_df[raw_df['Country'] == country].copy()\n",
        "    df_benchmarks = calculate_benchmarks(country_df)\n",
        "\n",
        "    print(\"Step 2b: Calculating round-based outliers...\")\n",
        "    num_bin_vars_for_calc = list(set(numerical + binary))\n",
        "    num_bin_vars_for_calc = [v for v in num_bin_vars_for_calc if v in country_df.columns]\n",
        "    cat_vars_for_calc = [v for v in categorical if v in country_df.columns]\n",
        "    df_round_outliers = calculate_round_outliers(country_df, num_bin_vars_for_calc, cat_vars_for_calc)\n",
        "\n",
        "    # --- Setup Excel Writer ---\n",
        "    filename = f\"Analysis by Group for {country}.xlsx\"\n",
        "    writer = pd.ExcelWriter(filename, engine=\"xlsxwriter\")\n",
        "    workbook = writer.book\n",
        "    bold = workbook.add_format({\"bold\": True})\n",
        "    plot_title_format = workbook.add_format({'bold': True, 'font_size': 11})\n",
        "    wrap_format = workbook.add_format({'text_wrap': True, 'valign': 'top'})\n",
        "    url_format = workbook.add_format({'font_color': 'blue', 'underline': 1})\n",
        "\n",
        "    # --- Writing Summary Sheets ---\n",
        "    print(\"Step 3: Writing summary sheets to Excel...\")\n",
        "\n",
        "    # --- REVISED: Add the Clustering sheet with the hyperlink and instructions ---\n",
        "    clustering_ws = workbook.add_worksheet(\"clustering\")\n",
        "\n",
        "    # Define your instructional text here. You can paste your final text inside the triple quotes.\n",
        "    instructional_text = \"\"\"Instructions for the Clustering Application :\n",
        "\n",
        "Introduction\n",
        "This application was developed for 100WEEKS to compare the performance of participating groups\n",
        "of women during the program. With the application, vulnerable groups can be identified and\n",
        "additional support can be provided.\n",
        "\n",
        "Access to the Application\n",
        "https://100weeks-clustering.streamlit.app/\n",
        "The application is developed to be user-friendly and it does not require any technical background to\n",
        "operate. After opening the application, a specific country can be selected via the drop-down menu.\n",
        "Based on this selection, visualizations are generated for the different survey rounds within that\n",
        "country. Plots are visualized for the baseline, round 2 (after 50 weeks), and round 100 (after 100\n",
        "weeks).\n",
        "\n",
        "Using the Application\n",
        "Every plot shows the results of a specific survey round. Each data point in the plot represents a\n",
        "participating group of women in the 100WEEKS program. By moving the computer mouse over a\n",
        "data point, the corresponding group number is displayed. In addition, a search bar is included for each\n",
        "plot where you can manually search for a specific group (e.g. GHA001). The group will appear in the\n",
        "plot, marked with a cross. The bottom left corner of the plots usually contains the most vulnerable\n",
        "groups, which score relatively low on poverty-related indicators. The top right corner shows groups\n",
        "that do relatively well on these indicators. It is important to mention that the plots are based on a\n",
        "simplified representation of the data. The dataset with multiple dimensions is reduced to two principal\n",
        "dimensions using Principal Component Analysis (PCA). This ensures that the results can be\n",
        "visualized in two-dimensional plots, but the plots only show an approximation of reality.\n",
        "\n",
        "Technical Setup\n",
        "The complete code and dataset are stored in the following GitHub repository:\n",
        "https://github.com/sam-jager/100weeks-clustering. The clustering.py file contains all of the python\n",
        "code. The file requirements.txt contains the Python libraries that are needed for the application to run.\n",
        "\n",
        "Adding a New Dataset\n",
        "When a new dataset is available for analysis, this can be uploaded with these steps. First, click the link\n",
        "to open the 100weeks-clustering GitHub repository. Click on the button ‘Add file’ and upload the new\n",
        "dataset. After this, open the clustering.py file and search for the following line of code at the\n",
        "beginning of the file:\n",
        "url = \"https://raw.githubusercontent.com/sam-jager/100weeks-clustering/main/central-tableau-export-2.0.csv\".\n",
        "Replace the filename (central-tableau-export-2.0.csv) in this URL with the new dataset’s filename.\n",
        "Save the changes with the ‘Commit changes’ button. The application automatically uses the new\n",
        "dataset.\n",
        "\"\"\"\n",
        "\n",
        "    # Set column width to be wide enough for instructions\n",
        "    clustering_ws.set_column('A:A', 80)\n",
        "\n",
        "    # Write the hyperlink to cell A1\n",
        "    clustering_ws.write_url(\n",
        "        'A1',\n",
        "        'https://100weeks-clustering.streamlit.app/',\n",
        "        url_format,\n",
        "        'Link to the Clustering Application'\n",
        "    )\n",
        "\n",
        "    # Write the instructional text to cell A3, applying the wrap format\n",
        "    clustering_ws.write('A3', instructional_text, wrap_format)\n",
        "\n",
        "    # Sheet: Trend-based Outlier Summary\n",
        "    summary_ws = workbook.add_worksheet(\"growth_outlier_summary\")\n",
        "    all_flags_rows = [\n",
        "        {'Groupnr': g, 'variable': var, 'Round': rnd}\n",
        "        for var, vf in flags.items()\n",
        "        for (g, rnd), tag in vf.items()\n",
        "    ]\n",
        "    if not all_flags_rows:\n",
        "        summary_ws.write('A1', 'No trend-based outliers found to summarize.')\n",
        "    else:\n",
        "        df_flags_long = pd.DataFrame(all_flags_rows)\n",
        "        df_grouped = (\n",
        "            df_flags_long.groupby(['Groupnr', 'Round'])['variable']\n",
        "            .apply(lambda x: '\\n'.join(sorted(x)))\n",
        "            .reset_index()\n",
        "        )\n",
        "        df_outlier_summary = df_grouped.pivot(\n",
        "            index='Groupnr', columns='Round', values='variable'\n",
        "        )\n",
        "        desired_round_order = [0, 1, 2, 3, 100, 102]\n",
        "        df_outlier_summary = df_outlier_summary.reindex(columns=desired_round_order).fillna('').reset_index()\n",
        "        summary_ws.set_column('A:A', 15);\n",
        "        summary_ws.set_column('B:G', 30)\n",
        "        for col_num, value in enumerate(df_outlier_summary.columns.values):\n",
        "            summary_ws.write(0, col_num, value, bold)\n",
        "        for row_num, row_data in enumerate(df_outlier_summary.itertuples(index=False), start=1):\n",
        "            summary_ws.write(row_num, 0, row_data[0])\n",
        "            for col_num, cell_data in enumerate(row_data[1:], start=1):\n",
        "                summary_ws.write(row_num, col_num, cell_data, wrap_format)\n",
        "\n",
        "    # Sheet: Round-based Outlier Summary\n",
        "    round_summary_ws = workbook.add_worksheet(\"round_outlier_summary\")\n",
        "    if df_round_outliers.empty:\n",
        "        round_summary_ws.write('A1', 'No round-based outliers found to summarize.')\n",
        "        print(\"No round-based outliers found.\")\n",
        "    else:\n",
        "        df_round_grouped = (\n",
        "            df_round_outliers.groupby(['Groupnr', 'Round'])['flag_description']\n",
        "            .apply(lambda x: '\\n'.join(sorted(x)))\n",
        "            .reset_index()\n",
        "        )\n",
        "        df_round_pivot = df_round_grouped.pivot(\n",
        "            index='Groupnr', columns='Round', values='flag_description'\n",
        "        )\n",
        "        desired_round_order = [0, 1, 2, 3, 100, 102]\n",
        "        df_round_pivot = df_round_pivot.reindex(columns=desired_round_order).fillna('').reset_index()\n",
        "        round_summary_ws.set_column('A:A', 15);\n",
        "        round_summary_ws.set_column('B:G', 40)\n",
        "        for col_num, value in enumerate(df_round_pivot.columns.values):\n",
        "            round_summary_ws.write(0, col_num, value, bold)\n",
        "        for row_num, row_data in enumerate(df_round_pivot.itertuples(index=False), start=1):\n",
        "            round_summary_ws.write(row_num, 0, row_data[0])\n",
        "            for col_num, cell_data in enumerate(row_data[1:], start=1):\n",
        "                round_summary_ws.write(row_num, col_num, cell_data, wrap_format)\n",
        "\n",
        "    # Sheet: Benchmark Summary\n",
        "    df_benchmarks.to_excel(writer, sheet_name=\"benchmark_summary\", index=True)\n",
        "\n",
        "    # --- EXISTING: Write Per-Group Sheets (Based on trend outliers) ---\n",
        "    print(f\"Step 4: Generating individual sheets for flagged groups...\")\n",
        "    for grp in groups:\n",
        "        rows = [{\"Round\": rnd, \"variable\": var, \"flag\": tag} for var, vf in flags.items() for (g, rnd), tag in\n",
        "                vf.items() if g == grp]\n",
        "        if not rows: continue\n",
        "        df_grp = pd.DataFrame(rows)\n",
        "        df_wide = df_grp.pivot(index=\"Round\", columns=\"variable\", values=\"flag\").reset_index().replace(\n",
        "            [np.inf, -np.inf, np.nan], None)\n",
        "        ws = workbook.add_worksheet(grp)\n",
        "        for c, col in enumerate(df_wide.columns): ws.write(0, c, col, bold)\n",
        "        for r, rec in enumerate(df_wide.itertuples(index=False), start=1):\n",
        "            for c, val in enumerate(rec): ws.write(r, c, val)\n",
        "        rowpos = len(df_wide) + 2\n",
        "        vars_flagged = sorted(df_grp[\"variable\"].unique())\n",
        "        for var in vars_flagged:\n",
        "            ws.write(rowpos, 0, f\"Analysis for variable: {var}\", plot_title_format);\n",
        "            rowpos += 2\n",
        "            buf1 = growth_plot(df_all, [grp], var, window=window)\n",
        "            if buf1: ws.insert_image(rowpos, 0, \"\", {\"image_data\": buf1, \"x_scale\": 0.75, \"y_scale\": 0.75})\n",
        "            buf2 = plot_groups_vs_overall(df_all, [grp], [var]).get(var)\n",
        "            if buf2: ws.insert_image(rowpos, 12, \"\", {\"image_data\": buf2, \"x_scale\": 0.75, \"y_scale\": 0.75})\n",
        "            rowpos += 22\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Success! Workbook saved to {filename}\")\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "all_variables = numerical + ordered_categorical + binary + categorical\n",
        "raw_df = df.copy()\n",
        "variables = all_variables\n",
        "# country = 'GHA'\n",
        "# groups = [f'GHA{i:03d}' for i in range(1, 11)]\n",
        "groups = raw_df[raw_df['Country'] == country]['Groupnr'].unique()\n",
        "xlsx_generator(raw_df, variables, country, groups, 3)"
      ]
    }
  ]
}